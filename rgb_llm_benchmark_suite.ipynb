{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTS AND DEPENDENCIES**"
      ],
      "metadata": {
        "id": "uXpZBhyx2kjj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_SdRVUYztms"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GLOBAL CONFIGURATION**"
      ],
      "metadata": {
        "id": "JjqI8Krt24oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "GROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/chen700564/RGB/master/data/\"\n",
        "s\n",
        "INFO_INT_FILE = \"en_int.json\"\n",
        "FACT_FILE = \"en_fact.json\"\n",
        "EN_FILE = \"en_refine.json\"\n",
        "ZH_FILE = \"zh_refine.json\""
      ],
      "metadata": {
        "id": "Jo2YbxgLzuq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation parameters\n",
        "SAMPLE_SIZE = 100  # Adjustable per evaluation\n",
        "MAX_DOCS = 5\n",
        "RANDOM_SEED = 42\n",
        "TEMPERATURE = 0.1  # Easy to change - 0.0 (deterministic) to 1.0 (creative)\n",
        "\n",
        "# RGB Noise Ratios - Following RGB paper methodology\n",
        "NOISE_RATIOS = [0.0, 0.2, 0.4]  # Information Integration noise ratios\n",
        "NOISE_RATIOS_FULL = [0.0, 0.2, 0.4, 0.6, 0.8]  # Full noise testing"
      ],
      "metadata": {
        "id": "LMTYLR-j0DHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS_TO_EVALUATE = [\n",
        "    \"llama-3.1-8b-instant\",\n",
        "    \"qwen/qwen3-32b\",\n",
        "    \"llama-3.3-70b-versatile\",\n",
        "    \"deepseek-r1-distill-llama-70b\",\n",
        "    \"gemma2-9b-it\"\n",
        "]"
      ],
      "metadata": {
        "id": "SkUXrh7q0Okc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    \"min_delay\": 0.5,\n",
        "    \"max_retries\": 3,\n",
        "    \"timeout\": 60,\n",
        "    \"max_context_length\": 12000,\n",
        "    \"temperature\": TEMPERATURE,\n",
        "    \"max_tokens\": 512\n",
        "}\n",
        "\n",
        "LAST_API_CALL = 0"
      ],
      "metadata": {
        "id": "7SpMMVpq0TCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UTILITY FUNCTIONS**"
      ],
      "metadata": {
        "id": "LGWWi5cM3CKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed_value=42):\n",
        "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    print(f\"Random seeds set to: {seed_value}\")\n",
        "\n",
        "def load_data(filename):\n",
        "    \"\"\"Load dataset from GitHub repository\"\"\"\n",
        "    url = BASE_URL + filename\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Failed to fetch {filename}. Status: {response.status_code}\")\n",
        "\n",
        "    text = response.text.strip()\n",
        "\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        data = []\n",
        "        for line in text.splitlines():\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                try:\n",
        "                    data.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "        return data\n",
        "\n",
        "def truncate_documents(docs, max_length=3500):\n",
        "    \"\"\"Truncate documents to fit context limits\"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    context = \"\\n\\n\".join(docs)\n",
        "    if len(context) <= max_length:\n",
        "        return docs\n",
        "\n",
        "    max_doc_length = max_length // len(docs)\n",
        "    truncated_docs = []\n",
        "\n",
        "    for doc in docs:\n",
        "        if len(doc) > max_doc_length:\n",
        "            truncated = doc[:max_doc_length]\n",
        "            last_period = truncated.rfind('.')\n",
        "            if last_period > max_doc_length * 0.8:\n",
        "                truncated_docs.append(doc[:last_period + 1])\n",
        "            else:\n",
        "                truncated_docs.append(doc[:max_doc_length] + \"...\")\n",
        "        else:\n",
        "            truncated_docs.append(doc)\n",
        "\n",
        "    return truncated_docs\n",
        "\n",
        "def query_model(question, docs, model, language=\"en\", system_prompt=None):\n",
        "    \"\"\"Universal model query function with error handling\"\"\"\n",
        "    global LAST_API_CALL\n",
        "\n",
        "    if not docs:\n",
        "        return \"API_ERROR: No documents provided\"\n",
        "\n",
        "    # Rate limiting\n",
        "    elapsed = time.time() - LAST_API_CALL\n",
        "    if elapsed < CONFIG[\"min_delay\"]:\n",
        "        time.sleep(CONFIG[\"min_delay\"] - elapsed)\n",
        "\n",
        "    # Prepare documents\n",
        "    truncated_docs = truncate_documents(docs, CONFIG[\"max_context_length\"])\n",
        "    docs_text = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(truncated_docs)])\n",
        "\n",
        "    # Use provided system prompt or default\n",
        "    if system_prompt is None:\n",
        "        if language == \"en\":\n",
        "            system_prompt = (\n",
        "                \"Answer the question based on the given documents. \"\n",
        "                \"If not found, say: 'I can not answer the question because of the insufficient information in documents.'\"\n",
        "            )\n",
        "        else:\n",
        "            system_prompt = \"请根据以下文档回答问题。如果无法回答，请回复：我无法回答这个问题，因为文档中没有足够的信息。\"\n",
        "\n",
        "    user_message = f\"Document:\\n{docs_text}\\n\\nQuestion:\\n{question}\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        \"temperature\": CONFIG[\"temperature\"],\n",
        "        \"max_tokens\": CONFIG[\"max_tokens\"]\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    for attempt in range(CONFIG[\"max_retries\"]):\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                GROQ_ENDPOINT,\n",
        "                headers=headers,\n",
        "                json=payload,\n",
        "                timeout=CONFIG[\"timeout\"]\n",
        "            )\n",
        "\n",
        "            LAST_API_CALL = time.time()\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if \"choices\" not in data or len(data[\"choices\"]) == 0:\n",
        "                if attempt == CONFIG[\"max_retries\"] - 1:\n",
        "                    return \"API_ERROR: Invalid response structure\"\n",
        "                continue\n",
        "\n",
        "            content = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "            return content if content else \"API_ERROR: Empty response\"\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            if attempt == CONFIG[\"max_retries\"] - 1:\n",
        "                return \"API_ERROR: Timeout\"\n",
        "            time.sleep(2 ** attempt)\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                if attempt == CONFIG[\"max_retries\"] - 1:\n",
        "                    return \"API_ERROR: Rate limited\"\n",
        "                time.sleep(5 * (attempt + 1))\n",
        "            elif e.response.status_code == 413:\n",
        "                return \"API_ERROR: Context too long\"\n",
        "            else:\n",
        "                return f\"API_ERROR: HTTP {e.response.status_code}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt == CONFIG[\"max_retries\"] - 1:\n",
        "                return f\"API_ERROR: {str(e)}\"\n",
        "            time.sleep(1)\n",
        "\n",
        "    return \"API_ERROR: Max retries exceeded\"\n",
        "\n",
        "def safe_sample(population, k):\n",
        "    \"\"\"Safely sample k items from population, handling edge cases\"\"\"\n",
        "    if not population:\n",
        "        return []\n",
        "    if k <= 0:\n",
        "        return []\n",
        "    if len(population) <= k:\n",
        "        return population[:]\n",
        "    return random.sample(population, k)"
      ],
      "metadata": {
        "id": "plOoljes0YiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFORMATION INTEGRATION EVALUATION**"
      ],
      "metadata": {
        "id": "BV3hICtJ3Oj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_information_integration_system_prompt():\n",
        "    \"\"\"RGB system instruction for Information Integration\"\"\"\n",
        "    return \"\"\"You are an accurate and reliable AI assistant that can answer questions with the help of external documents. Please note that external documents may contain noisy or factually incorrect information. If the information in the document contains the correct answer, you will give an accurate answer. If the information in the document does not contain the answer, you will generate 'I can not answer the question because of the insufficient information in documents.' If there are inconsistencies with the facts in some of the documents, please generate the response 'There are factual errors in the provided documents.' and provide the correct answer.\"\"\"\n",
        "\n",
        "def prepare_documents(positive_docs, negative_docs, noise_ratio=0.0, max_docs=5):\n",
        "    \"\"\"\n",
        "    Prepare documents as per RGB methodology\n",
        "    noise_ratio: proportion of negative documents (0.0 = no noise)\n",
        "    \"\"\"\n",
        "    all_positive = []\n",
        "    for doc_group in positive_docs:\n",
        "        if isinstance(doc_group, list):\n",
        "            all_positive.extend(doc_group)\n",
        "        else:\n",
        "            all_positive.append(doc_group)\n",
        "\n",
        "    all_negative = []\n",
        "    for doc in negative_docs:\n",
        "        all_negative.append(doc)\n",
        "\n",
        "    # Select documents total\n",
        "    num_negative = int(max_docs * noise_ratio)\n",
        "    num_positive = max_docs - num_negative\n",
        "\n",
        "    # Safely sample documents\n",
        "    selected_positive = safe_sample(all_positive, min(num_positive, len(all_positive)))\n",
        "    selected_negative = safe_sample(all_negative, min(num_negative, len(all_negative)))\n",
        "\n",
        "    # Fill remaining slots if needed\n",
        "    while len(selected_positive) + len(selected_negative) < max_docs:\n",
        "        if len(all_positive) > len(selected_positive):\n",
        "            selected_positive.append(random.choice(all_positive))\n",
        "        elif len(all_negative) > len(selected_negative):\n",
        "            selected_negative.append(random.choice(all_negative))\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Combine and shuffle\n",
        "    all_docs = selected_positive + selected_negative\n",
        "    random.shuffle(all_docs)\n",
        "\n",
        "    return all_docs[:max_docs]\n",
        "\n",
        "def evaluate_information_integration(response, expected_answers):\n",
        "    \"\"\"\n",
        "    Evaluate Information Integration response\n",
        "    expected_answers format: [answer_list1, answer_list2, ...]\n",
        "    \"\"\"\n",
        "    if \"API_ERROR\" in response:\n",
        "        return \"ERROR\"\n",
        "\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    # Check if response contains rejection\n",
        "    rejection_phrases = [\n",
        "        \"insufficient information\",\n",
        "        \"cannot answer\",\n",
        "        \"can not answer\",\n",
        "        \"not enough information\",\n",
        "        \"unable to answer\",\n",
        "        \"i can not answer the question because of the insufficient information in documents\"\n",
        "    ]\n",
        "\n",
        "    if any(phrase in response_lower for phrase in rejection_phrases):\n",
        "        return \"REJECTED\"\n",
        "\n",
        "    # Check factual errors claim\n",
        "    factual_errors_patterns = [\n",
        "        \"there are factual errors in the provided documents\",\n",
        "        \"factual errors in the provided documents\"\n",
        "    ]\n",
        "\n",
        "    has_factual_errors_claim = any(pattern in response_lower for pattern in factual_errors_patterns)\n",
        "\n",
        "    # Check each answer component\n",
        "    components_found = 0\n",
        "\n",
        "    for answer_group in expected_answers:\n",
        "        component_found = False\n",
        "        for answer_variant in answer_group:\n",
        "            if answer_variant.lower() in response_lower:\n",
        "                component_found = True\n",
        "                break\n",
        "        if component_found:\n",
        "            components_found += 1\n",
        "\n",
        "    total_components = len(expected_answers)\n",
        "\n",
        "    # Determine evaluation based on components found\n",
        "    if components_found == total_components:\n",
        "        return \"FULL_SUCCESS\"\n",
        "    elif components_found > 0:\n",
        "        return \"PARTIAL_SUCCESS\"\n",
        "    else:\n",
        "        if has_factual_errors_claim:\n",
        "            return \"FACTUAL_ERROR_DETECTED\"\n",
        "        else:\n",
        "            return \"FAILURE\"\n",
        "\n",
        "def evaluate_rgb_information_integration(dataset, model, sample_size, max_docs, noise_ratio):\n",
        "    \"\"\"RGB paper-style Information Integration evaluation for single noise ratio\"\"\"\n",
        "    print(f\"\\nEvaluating {model} for RGB Information Integration (Noise: {noise_ratio})\")\n",
        "\n",
        "    # Filter samples with required document types\n",
        "    filtered_samples = []\n",
        "    for sample in dataset:\n",
        "        if (sample.get(\"positive\") and sample.get(\"negative\") and sample.get(\"answer\")):\n",
        "            # Check minimum document requirements\n",
        "            if (len(sample.get(\"positive\", [])) >= 2 and\n",
        "                len(sample.get(\"negative\", [])) >= 2):\n",
        "                filtered_samples.append(sample)\n",
        "        if len(filtered_samples) >= sample_size:\n",
        "            break\n",
        "\n",
        "    if len(filtered_samples) < sample_size:\n",
        "        print(f\"Warning: Only found {len(filtered_samples)} quality samples (target: {sample_size})\")\n",
        "\n",
        "    actual_sample_size = min(sample_size, len(filtered_samples))\n",
        "    samples_to_process = filtered_samples[:actual_sample_size]\n",
        "\n",
        "    results = []\n",
        "    stats = {\n",
        "        \"FULL_SUCCESS\": 0,\n",
        "        \"PARTIAL_SUCCESS\": 0,\n",
        "        \"FAILURE\": 0,\n",
        "        \"REJECTED\": 0,\n",
        "        \"FACTUAL_ERROR_DETECTED\": 0,\n",
        "        \"ERROR\": 0,\n",
        "        \"total\": 0\n",
        "    }\n",
        "\n",
        "    # Progress tracking\n",
        "    progress_bar = tqdm(total=actual_sample_size, desc=f\"{model} (noise={noise_ratio})\")\n",
        "\n",
        "    for i, sample in enumerate(samples_to_process):\n",
        "        try:\n",
        "            query = sample[\"query\"]\n",
        "            expected_answers = sample[\"answer\"]\n",
        "\n",
        "            # Prepare documents with noise ratio\n",
        "            docs = prepare_documents(\n",
        "                sample[\"positive\"],\n",
        "                sample[\"negative\"],\n",
        "                noise_ratio=noise_ratio,\n",
        "                max_docs=max_docs\n",
        "            )\n",
        "\n",
        "            if not docs:\n",
        "                continue\n",
        "\n",
        "            # Query model\n",
        "            system_prompt = get_information_integration_system_prompt()\n",
        "            response = query_model(query, docs, model, system_prompt=system_prompt)\n",
        "            evaluation = evaluate_information_integration(response, expected_answers)\n",
        "\n",
        "            # Update statistics\n",
        "            stats[evaluation] += 1\n",
        "            stats[\"total\"] += 1\n",
        "\n",
        "            # Store result\n",
        "            result = {\n",
        "                \"sample_id\": sample.get('id', i + 1),\n",
        "                \"model\": model,\n",
        "                \"query\": query,\n",
        "                \"response\": response,\n",
        "                \"evaluation\": evaluation,\n",
        "                \"expected_answers\": expected_answers,\n",
        "                \"components_expected\": len(expected_answers),\n",
        "                \"noise_ratio\": noise_ratio,\n",
        "                \"docs_used\": len(docs),\n",
        "                \"temperature\": CONFIG[\"temperature\"],\n",
        "                \"documents\": \" ||| \".join(docs[:2])\n",
        "            }\n",
        "\n",
        "            results.append(result)\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError processing sample {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    # Calculate and display metrics\n",
        "    total = stats[\"total\"]\n",
        "    if total > 0:\n",
        "        full_success_rate = (stats[\"FULL_SUCCESS\"] / total * 100)\n",
        "        partial_success_rate = (stats[\"PARTIAL_SUCCESS\"] / total * 100)\n",
        "\n",
        "        print(f\"  Noise {noise_ratio}: Full: {full_success_rate:5.1f}%, \"\n",
        "              f\"Partial: {partial_success_rate:5.1f}%, \"\n",
        "              f\"Total: {full_success_rate + partial_success_rate:5.1f}%\")\n",
        "\n",
        "    return stats, results"
      ],
      "metadata": {
        "id": "5khClDjB0nTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FACTUAL ACCURACY EVALUATION**"
      ],
      "metadata": {
        "id": "4x9AEsej3YJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_factual_system_prompt():\n",
        "    return \"\"\"You are an accurate and reliable AI assistant that can answer questions with the help of external documents. Please note that external documents may contain noisy or factually incorrect information. If the information in the document contains the correct answer, you will give an accurate answer. If the information in the document does not contain the answer, you will generate 'I can not answer the question because of the insufficient information in documents.'. If there are inconsistencies with the facts in some of the documents, please generate the response 'There are factual errors in the provided documents.' and provide the correct answer.\"\"\"\n",
        "\n",
        "def normalize_answer(answer):\n",
        "    \"\"\"Convert answer to string format, handling lists and other types\"\"\"\n",
        "    if answer is None:\n",
        "        return \"\"\n",
        "\n",
        "    if isinstance(answer, list):\n",
        "        if len(answer) > 0:\n",
        "            return str(answer[0])\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "    return str(answer)\n",
        "\n",
        "def evaluate_answer(response, correct_answer, fake_answer):\n",
        "    \"\"\"Enhanced evaluation function fixing external knowledge detection and logic issues\"\"\"\n",
        "    if \"API_ERROR\" in response:\n",
        "        return \"ERROR\"\n",
        "\n",
        "    # Normalize answers\n",
        "    correct_answer = normalize_answer(correct_answer)\n",
        "    fake_answer = normalize_answer(fake_answer)\n",
        "\n",
        "    response_lower = response.lower()\n",
        "    correct_lower = correct_answer.lower()\n",
        "    fake_lower = fake_answer.lower()\n",
        "\n",
        "    # ENHANCED external knowledge patterns (CRITICAL FIX)\n",
        "    external_knowledge_patterns = [\n",
        "        \"based on external knowledge\",\n",
        "        \"based on general knowledge\",\n",
        "        \"according to external information\",\n",
        "        \"based on external sources\",\n",
        "        \"according to multiple reliable sources\",\n",
        "        \"based on multiple sources\",\n",
        "        \"multiple reliable sources\",\n",
        "        \"however, the correct answer is\",\n",
        "        \"but actually\",\n",
        "        \"in reality\",\n",
        "        \"the actual winner was\",\n",
        "        \"according to multiple sources\",\n",
        "        \"based on common knowledge\",\n",
        "        \"from what i know\",\n",
        "        \"as far as i know\",\n",
        "        \"to my knowledge\",\n",
        "        \"however, based on\",\n",
        "        \"but the actual\",\n",
        "        \"the real answer is\",\n",
        "        \"according to external\",\n",
        "        \"based on external\",\n",
        "        \"external knowledge\",\n",
        "        \"multiple sources\",\n",
        "        \"reliable sources\"\n",
        "    ]\n",
        "\n",
        "    # CHECK EXTERNAL KNOWLEDGE FIRST (before other evaluations)\n",
        "    uses_external_knowledge = any(pattern in response_lower for pattern in external_knowledge_patterns)\n",
        "    if uses_external_knowledge:\n",
        "        return \"REJECTED\"  # Model cheated by using external knowledge\n",
        "\n",
        "    # Check for structured responses from the prompt\n",
        "    insufficient_info_patterns = [\n",
        "        \"i can not answer the question because of the insufficient information in documents\",\n",
        "        \"i cannot answer the question because of the insufficient information in documents\",\n",
        "        \"i can not answer the question because of insufficient information in documents\",\n",
        "        \"i cannot answer the question because of insufficient information in documents\",\n",
        "        \"insufficient information in documents\",\n",
        "        \"i can not answer\",\n",
        "        \"i cannot answer\"\n",
        "    ]\n",
        "\n",
        "    factual_errors_patterns = [\n",
        "        \"there are factual errors in the provided documents\",\n",
        "        \"factual errors in the provided documents\",\n",
        "        \"there are factual errors\",\n",
        "        \"factual errors\"\n",
        "    ]\n",
        "\n",
        "    # Check for different response types\n",
        "    is_insufficient_info = any(pattern in response_lower for pattern in insufficient_info_patterns)\n",
        "    has_factual_errors_claim = any(pattern in response_lower for pattern in factual_errors_patterns)\n",
        "\n",
        "    # Check for answers\n",
        "    has_correct = correct_lower in response_lower if correct_lower else False\n",
        "    has_fake = fake_lower in response_lower if fake_lower else False\n",
        "\n",
        "    # ENHANCED evaluation logic\n",
        "    if is_insufficient_info:\n",
        "        return \"REJECTED\"\n",
        "    elif has_factual_errors_claim:\n",
        "        # Model detected errors - IMPROVED logic\n",
        "        if has_correct and not has_fake:\n",
        "            return \"CORRECT\"  # Detected errors, gave correct answer\n",
        "        elif has_correct and has_fake:\n",
        "            # If model detected errors AND provided correct answer prominently, count as CORRECT\n",
        "            # Check if correct answer appears more prominently or is stated as the final answer\n",
        "            response_parts = response_lower.split('.')\n",
        "            for part in reversed(response_parts[-3:]):  # Check last 3 sentences\n",
        "                if correct_lower in part and fake_lower not in part:\n",
        "                    return \"CORRECT\"  # Correct answer stated clearly at end\n",
        "            return \"PARTIAL\"  # Both answers present without clear preference\n",
        "        elif has_fake and not has_correct:\n",
        "            return \"INCORRECT\"  # Detected errors, but gave wrong answer\n",
        "        else:\n",
        "            return \"REJECTED\"  # Detected errors, gave no clear answer\n",
        "    else:\n",
        "        # Standard evaluation for direct answers\n",
        "        if has_correct and not has_fake:\n",
        "            return \"CORRECT\"\n",
        "        elif has_fake and not has_correct:\n",
        "            return \"INCORRECT\"\n",
        "        elif has_correct and has_fake:\n",
        "            return \"PARTIAL\"\n",
        "        else:\n",
        "            return \"NO_ANSWER\"\n",
        "\n",
        "def create_document_sets(sample, max_docs):\n",
        "    \"\"\"Create different document sets for RGB evaluation with robust sampling\"\"\"\n",
        "    positive_docs = sample.get(\"positive\", [])\n",
        "    negative_docs = sample.get(\"negative\", [])\n",
        "    wrong_docs = sample.get(\"positive_wrong\", [])\n",
        "\n",
        "    document_sets = {}\n",
        "\n",
        "    # All positive (supporting evidence) - need at least 3 docs\n",
        "    if len(positive_docs) >= 3:\n",
        "        sample_size = min(max_docs, len(positive_docs))\n",
        "        document_sets[\"all_positive\"] = safe_sample(positive_docs, sample_size)\n",
        "\n",
        "    # All negative (irrelevant) - need at least 3 docs\n",
        "    if len(negative_docs) >= 3:\n",
        "        sample_size = min(max_docs, len(negative_docs))\n",
        "        document_sets[\"all_negative\"] = safe_sample(negative_docs, sample_size)\n",
        "\n",
        "    # All wrong (misinformation) - need at least 3 docs\n",
        "    if len(wrong_docs) >= 3:\n",
        "        sample_size = min(max_docs, len(wrong_docs))\n",
        "        document_sets[\"all_wrong\"] = safe_sample(wrong_docs, sample_size)\n",
        "\n",
        "    # Mixed (realistic scenario) - need at least max_docs total\n",
        "    all_docs = positive_docs + negative_docs + wrong_docs\n",
        "    if len(all_docs) >= max_docs:\n",
        "        document_sets[\"mixed\"] = safe_sample(all_docs, max_docs)\n",
        "\n",
        "    # Positive + Wrong mix (challenging scenario) - need at least 2 of each\n",
        "    if len(positive_docs) >= 2 and len(wrong_docs) >= 2:\n",
        "        # Take 2-3 positive and 2-3 wrong to make max_docs total\n",
        "        pos_count = min(3, len(positive_docs), max_docs // 2 + 1)\n",
        "        wrong_count = min(max_docs - pos_count, len(wrong_docs))\n",
        "\n",
        "        if pos_count + wrong_count >= 4:  # Minimum viable set\n",
        "            pos_subset = safe_sample(positive_docs, pos_count)\n",
        "            wrong_subset = safe_sample(wrong_docs, wrong_count)\n",
        "            combined = pos_subset + wrong_subset\n",
        "            random.shuffle(combined)  # Randomize order\n",
        "            document_sets[\"pos_wrong_mix\"] = combined\n",
        "\n",
        "    return document_sets\n",
        "\n",
        "def evaluate_rgb_factual_accuracy(dataset, model, sample_size, max_docs):\n",
        "    \"\"\"RGB paper-style factual accuracy evaluation\"\"\"\n",
        "    print(f\"\\nEvaluating {model} for RGB factual accuracy\")\n",
        "    print(f\"Sample size: {sample_size}, Max docs per question: {max_docs}\")\n",
        "\n",
        "    # Filter samples with required document types\n",
        "    filtered_samples = []\n",
        "    for sample in dataset:\n",
        "        if (sample.get(\"positive\") and sample.get(\"negative\") and\n",
        "            sample.get(\"positive_wrong\") and sample.get(\"answer\") and sample.get(\"fakeanswer\")):\n",
        "            # Check minimum document requirements\n",
        "            if (len(sample.get(\"positive\", [])) >= 2 and\n",
        "                len(sample.get(\"negative\", [])) >= 2 and\n",
        "                len(sample.get(\"positive_wrong\", [])) >= 2):\n",
        "                filtered_samples.append(sample)\n",
        "        if len(filtered_samples) >= sample_size:\n",
        "            break\n",
        "\n",
        "    if len(filtered_samples) < sample_size:\n",
        "        print(f\"Warning: Only found {len(filtered_samples)} quality samples (target: {sample_size})\")\n",
        "\n",
        "    actual_sample_size = min(sample_size, len(filtered_samples))\n",
        "    samples_to_process = filtered_samples[:actual_sample_size]\n",
        "\n",
        "    results = []\n",
        "    scenario_stats = {}\n",
        "\n",
        "    # Initialize statistics\n",
        "    scenarios = [\"all_positive\", \"all_negative\", \"all_wrong\", \"mixed\", \"pos_wrong_mix\"]\n",
        "    eval_types = [\"CORRECT\", \"INCORRECT\", \"REJECTED\", \"PARTIAL\", \"NO_ANSWER\", \"ERROR\"]\n",
        "\n",
        "    for scenario in scenarios:\n",
        "        scenario_stats[scenario] = {eval_type: 0 for eval_type in eval_types}\n",
        "        scenario_stats[scenario][\"total\"] = 0\n",
        "\n",
        "    # Count available scenarios first\n",
        "    available_scenarios = set()\n",
        "    total_evaluations = 0\n",
        "\n",
        "    for sample in samples_to_process:\n",
        "        document_sets = create_document_sets(sample, max_docs)\n",
        "        available_scenarios.update(document_sets.keys())\n",
        "        total_evaluations += len(document_sets)\n",
        "\n",
        "    print(f\"Available scenarios: {sorted(available_scenarios)}\")\n",
        "    print(f\"Total evaluations: {total_evaluations}\")\n",
        "\n",
        "    # Progress tracking\n",
        "    progress_bar = tqdm(total=total_evaluations, desc=f\"{model}\")\n",
        "\n",
        "    for i, sample in enumerate(samples_to_process):\n",
        "        try:\n",
        "            query = sample[\"query\"]\n",
        "            document_sets = create_document_sets(sample, max_docs)\n",
        "\n",
        "            for scenario_name, docs in document_sets.items():\n",
        "                if not docs:  # Skip empty document sets\n",
        "                    continue\n",
        "\n",
        "                # Query model\n",
        "                system_prompt = get_factual_system_prompt()\n",
        "                response = query_model(query, docs, model, system_prompt=system_prompt)\n",
        "                evaluation = evaluate_answer(response, sample[\"answer\"], sample[\"fakeanswer\"])\n",
        "\n",
        "                # Update statistics\n",
        "                scenario_stats[scenario_name][evaluation] += 1\n",
        "                scenario_stats[scenario_name][\"total\"] += 1\n",
        "\n",
        "                # Store result\n",
        "                result = {\n",
        "                    \"sample_id\": i + 1,\n",
        "                    \"model\": model,\n",
        "                    \"query\": query,\n",
        "                    \"scenario\": scenario_name,\n",
        "                    \"response\": response,\n",
        "                    \"evaluation\": evaluation,\n",
        "                    \"correct_answer\": normalize_answer(sample[\"answer\"]),\n",
        "                    \"fake_answer\": normalize_answer(sample[\"fakeanswer\"]),\n",
        "                    \"docs_used\": len(docs),\n",
        "                    \"documents\": \" ||| \".join(docs[:2])\n",
        "                }\n",
        "\n",
        "                results.append(result)\n",
        "                progress_bar.update(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError processing sample {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    # Calculate and display metrics\n",
        "    print(f\"\\n{model} RGB Results:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for scenario in scenarios:\n",
        "        stats = scenario_stats[scenario]\n",
        "        total = stats[\"total\"]\n",
        "\n",
        "        if total > 0:\n",
        "            correct = stats[\"CORRECT\"]\n",
        "            incorrect = stats[\"INCORRECT\"]\n",
        "            rejected = stats[\"REJECTED\"]\n",
        "            partial = stats[\"PARTIAL\"]\n",
        "            no_answer = stats[\"NO_ANSWER\"]\n",
        "            error = stats[\"ERROR\"]\n",
        "\n",
        "            accuracy = (correct / total * 100)\n",
        "            error_rate = (incorrect / total * 100)\n",
        "            rejection_rate = (rejected / total * 100)\n",
        "\n",
        "            print(f\"{scenario:15s}: Acc: {accuracy:5.1f}% ({correct:2d}/{total:2d}), \"\n",
        "                  f\"Err: {error_rate:5.1f}%, Rej: {rejection_rate:5.1f}%\")\n",
        "        else:\n",
        "            print(f\"{scenario:15s}: No samples available\")\n",
        "\n",
        "    return scenario_stats, results"
      ],
      "metadata": {
        "id": "58ibRKR40woR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOISE ROBUSTNESS EVALUATION**"
      ],
      "metadata": {
        "id": "4kaJ402C3kxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answers(answer_structure, language):\n",
        "    \"\"\"Extract answers from data structure\"\"\"\n",
        "    answers = []\n",
        "\n",
        "    for item in answer_structure:\n",
        "        if isinstance(item, list):\n",
        "            answers.extend(item)\n",
        "        elif isinstance(item, str):\n",
        "            answers.append(item)\n",
        "\n",
        "    if language == \"en\":\n",
        "        return [ans.lower().strip() for ans in answers]\n",
        "    else:\n",
        "        return [ans.strip() for ans in answers]\n",
        "\n",
        "def add_noise(positive_docs, negative_docs, noise_ratio, sample_id=0):\n",
        "    \"\"\"Add noise documents based on ratio with consistent seeding\"\"\"\n",
        "    # Set sample-specific seed to ensure same question gets same docs across noise ratios\n",
        "    sample_seed = 42 + sample_id\n",
        "    random.seed(sample_seed)\n",
        "    np.random.seed(sample_seed)\n",
        "\n",
        "    if noise_ratio == 0:\n",
        "        selected_docs = random.sample(positive_docs, min(MAX_DOCS, len(positive_docs)))\n",
        "        while len(selected_docs) < MAX_DOCS and len(selected_docs) < len(positive_docs):\n",
        "            remaining = [doc for doc in positive_docs if doc not in selected_docs]\n",
        "            if remaining:\n",
        "                selected_docs.append(random.choice(remaining))\n",
        "            else:\n",
        "                break\n",
        "    else:\n",
        "        num_negatives = int(MAX_DOCS * noise_ratio)\n",
        "        num_positives = MAX_DOCS - num_negatives\n",
        "\n",
        "        selected_docs = []\n",
        "\n",
        "        if num_positives > 0 and positive_docs:\n",
        "            pos_sample = random.sample(positive_docs, min(num_positives, len(positive_docs)))\n",
        "            selected_docs.extend(pos_sample)\n",
        "\n",
        "        if num_negatives > 0 and negative_docs:\n",
        "            neg_sample = random.sample(negative_docs, min(num_negatives, len(negative_docs)))\n",
        "            selected_docs.extend(neg_sample)\n",
        "\n",
        "        while len(selected_docs) < MAX_DOCS and len(selected_docs) < len(positive_docs):\n",
        "            remaining_pos = [doc for doc in positive_docs if doc not in selected_docs]\n",
        "            if remaining_pos:\n",
        "                selected_docs.append(random.choice(remaining_pos))\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    selected_docs = selected_docs[:MAX_DOCS]\n",
        "\n",
        "    # Use noise-ratio specific seed for shuffling\n",
        "    shuffle_seed = sample_seed + int(noise_ratio * 100)\n",
        "    random.seed(shuffle_seed)\n",
        "    random.shuffle(selected_docs)\n",
        "\n",
        "    return selected_docs\n",
        "\n",
        "def check_correct(predicted, true_answers, language):\n",
        "    \"\"\"General answer checking without hardcoding\"\"\"\n",
        "    if not predicted.strip() or \"API_ERROR\" in predicted:\n",
        "        return False\n",
        "\n",
        "    rejection_phrases = [\n",
        "        \"i can not answer\", \"insufficient information\", \"cannot answer\",\n",
        "        \"don't know\", \"not enough information\", \"unable to answer\",\n",
        "        \"not found\", \"unclear\", \"not available\", \"not provided\"\n",
        "    ]\n",
        "\n",
        "    if language == \"zh\":\n",
        "        rejection_phrases.extend([\n",
        "            \"无法回答\", \"信息不足\", \"不知道\", \"无法\", \"不能回答\",\n",
        "            \"文档中没有\", \"不确定\", \"不清楚\", \"没有提供\", \"找不到\"\n",
        "        ])\n",
        "\n",
        "    predicted_lower = predicted.lower() if language == \"en\" else predicted\n",
        "    for phrase in rejection_phrases:\n",
        "        if phrase in predicted_lower:\n",
        "            return False\n",
        "\n",
        "    processed_pred = predicted.lower().strip() if language == \"en\" else predicted.strip()\n",
        "\n",
        "    for ans in true_answers:\n",
        "        if len(ans) >= 1:\n",
        "            ans_clean = ans.lower().strip() if language == \"en\" else ans.strip()\n",
        "\n",
        "            # Strategy 1: Direct substring matching\n",
        "            if ans_clean in processed_pred:\n",
        "                return True\n",
        "\n",
        "            # Strategy 2: Extract all tokens and numbers\n",
        "            def extract_tokens(text):\n",
        "                tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "                return [t for t in tokens if len(t) > 0]\n",
        "\n",
        "            pred_tokens = extract_tokens(processed_pred)\n",
        "            ans_tokens = extract_tokens(ans_clean)\n",
        "\n",
        "            # Check if all answer tokens appear in prediction\n",
        "            if ans_tokens and all(token in pred_tokens for token in ans_tokens):\n",
        "                return True\n",
        "\n",
        "            # Strategy 3: Number matching\n",
        "            pred_numbers = re.findall(r'\\d+(?:\\.\\d+)?', processed_pred)\n",
        "            ans_numbers = re.findall(r'\\d+(?:\\.\\d+)?', ans_clean)\n",
        "\n",
        "            if ans_numbers and all(num in pred_numbers for num in ans_numbers):\n",
        "                return True\n",
        "\n",
        "            # Strategy 4: Word boundary matching for short answers\n",
        "            if len(ans_clean) <= 4:\n",
        "                pattern = r'\\b' + re.escape(ans_clean) + r'\\b'\n",
        "                if re.search(pattern, processed_pred):\n",
        "                    return True\n",
        "\n",
        "            # Strategy 5: Multi-word partial matching\n",
        "            if len(ans_clean.split()) > 1:\n",
        "                ans_words = [w for w in ans_clean.split() if len(w) > 2]\n",
        "                if ans_words and all(word in processed_pred for word in ans_words):\n",
        "                    return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def eval_noise_test(dataset, model, language, sample_size, base_seed=42):\n",
        "    \"\"\"Evaluate model with noise testing using consistent seeding\"\"\"\n",
        "    results = {}\n",
        "    details = []\n",
        "\n",
        "    # Set initial seed for dataset filtering\n",
        "    set_seeds(base_seed)\n",
        "\n",
        "    filtered_dataset = []\n",
        "    for sample in dataset[:sample_size]:\n",
        "        if (len(sample.get('positive', [])) >= 2 and\n",
        "            len(sample.get('negative', [])) >= 3):\n",
        "            filtered_dataset.append(sample)\n",
        "\n",
        "    print(f\"Filtered to {len(filtered_dataset)} quality samples from {min(sample_size, len(dataset))}\")\n",
        "\n",
        "    total_evaluations = len(NOISE_RATIOS_FULL) * len(filtered_dataset)\n",
        "    progress_bar = tqdm(total=total_evaluations, desc=f\"{model} {language.upper()}\")\n",
        "\n",
        "    for ratio in NOISE_RATIOS_FULL:\n",
        "        correct = 0\n",
        "        total = len(filtered_dataset)\n",
        "        api_errors = 0\n",
        "\n",
        "        for i, sample in enumerate(filtered_dataset):\n",
        "            question = sample[\"query\"]\n",
        "            true_answers = extract_answers(sample[\"answer\"], language)\n",
        "\n",
        "            # Use sample ID to ensure consistent document selection\n",
        "            docs = add_noise(sample[\"positive\"], sample[\"negative\"], ratio, sample_id=i)\n",
        "            predicted = query_model(question, docs, model, language)\n",
        "\n",
        "            if \"API_ERROR\" in predicted:\n",
        "                api_errors += 1\n",
        "                is_correct = False\n",
        "            else:\n",
        "                is_correct = check_correct(predicted, true_answers, language)\n",
        "\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "            neg_docs_used = sum(1 for doc in docs if doc in sample['negative'])\n",
        "            actual_noise_ratio = neg_docs_used / len(docs) if docs else 0\n",
        "\n",
        "            details.append({\n",
        "                \"sample_id\": i + 1,\n",
        "                \"model\": model,\n",
        "                \"language\": language,\n",
        "                \"noise_ratio\": ratio,\n",
        "                \"actual_noise_ratio\": actual_noise_ratio,\n",
        "                \"question\": question,\n",
        "                \"predicted\": predicted,\n",
        "                \"true_answers\": \"; \".join([str(ans) for ans in sample[\"answer\"]]),\n",
        "                \"extracted_answers\": \"; \".join(true_answers),\n",
        "                \"correct\": is_correct,\n",
        "                \"docs_used\": len(docs),\n",
        "                \"pos_docs_available\": len(sample[\"positive\"]),\n",
        "                \"neg_docs_available\": len(sample[\"negative\"]),\n",
        "                \"pos_docs_used\": sum(1 for doc in docs if doc in sample['positive']),\n",
        "                \"neg_docs_used\": neg_docs_used\n",
        "            })\n",
        "\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        accuracy = (correct / total) * 100\n",
        "        results[ratio] = accuracy\n",
        "\n",
        "        error_info = f\" (API errors: {api_errors})\" if api_errors > 0 else \"\"\n",
        "        print(f\"{model} {language.upper()} | Noise {int(ratio*100)}%: {accuracy:.1f}% ({correct}/{total}){error_info}\")\n",
        "\n",
        "    progress_bar.close()\n",
        "    return results, details"
      ],
      "metadata": {
        "id": "TRWL4w8N05rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NEGATIVE REJECTION EVALUATION**"
      ],
      "metadata": {
        "id": "_Dr2rIGF3nJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rgb_system_prompt():\n",
        "    \"\"\"Same simple system prompt as noise ratio test\"\"\"\n",
        "    return (\n",
        "        \"Answer the question based on the given documents. \"\n",
        "        \"If not found, say: 'I can not answer the question because of the insufficient information in documents.'\"\n",
        "    )\n",
        "\n",
        "def check_exact_rejection(predicted):\n",
        "    \"\"\"RGB paper exact matching for rejection (Rej metric)\"\"\"\n",
        "    if not predicted.strip() or \"API_ERROR\" in predicted:\n",
        "        return False\n",
        "\n",
        "    # Exact phrase from RGB paper\n",
        "    exact_phrase = \"I can not answer the question because of the insufficient information in documents.\"\n",
        "    return exact_phrase.lower() in predicted.lower()\n",
        "\n",
        "def check_semantic_rejection(predicted):\n",
        "    \"\"\"RGB paper semantic rejection detection (Rej* metric without ChatGPT)\"\"\"\n",
        "    if not predicted.strip() or \"API_ERROR\" in predicted:\n",
        "        return False\n",
        "\n",
        "    # More flexible rejection indicators\n",
        "    rejection_patterns = [\n",
        "        r\"can\\s*not\\s+answer.*insufficient\",\n",
        "        r\"cannot\\s+answer.*insufficient\",\n",
        "        r\"unable\\s+to\\s+answer.*insufficient\",\n",
        "        r\"don[\\'']?t\\s+have\\s+enough.*information\",\n",
        "        r\"not\\s+enough\\s+information\",\n",
        "        r\"insufficient\\s+information\",\n",
        "        r\"information.*not.*sufficient\",\n",
        "        r\"documents.*do\\s+not\\s+contain\",\n",
        "        r\"no\\s+relevant\\s+information\",\n",
        "        r\"cannot\\s+find.*answer\",\n",
        "        r\"unable\\s+to\\s+find.*answer\",\n",
        "        r\"information.*missing\",\n",
        "        r\"lack.*information\"\n",
        "    ]\n",
        "\n",
        "    predicted_lower = predicted.lower()\n",
        "\n",
        "    # Check for rejection patterns\n",
        "    for pattern in rejection_patterns:\n",
        "        if re.search(pattern, predicted_lower):\n",
        "            return True\n",
        "\n",
        "    # Check for answer-like content (if it contains specific facts, likely not a rejection)\n",
        "    answer_indicators = [\n",
        "        r\"\\b\\d{4}\\b\",  # Years\n",
        "        r\"\\b\\d+\\s*(million|billion|thousand)\\b\",  # Numbers with units\n",
        "        r\"\\b[A-Z][a-z]+\\s+[A-Z][a-z]+\\b\",  # Proper names\n",
        "        r\"according\\s+to\",  # Reference phrases\n",
        "        r\"based\\s+on\",\n",
        "        r\"the\\s+answer\\s+is\",\n",
        "        r\"was\\s+(born|awarded|won|defeated)\"\n",
        "    ]\n",
        "\n",
        "    for pattern in answer_indicators:\n",
        "        if re.search(pattern, predicted):\n",
        "            return False  # Contains answer-like content, probably not a rejection\n",
        "\n",
        "    return False\n",
        "\n",
        "def should_be_rejected_rgb(question, docs):\n",
        "    \"\"\"\n",
        "    RGB paper approach: For negative rejection testbed,\n",
        "    ALL documents should be negative (noisy), so ALL should be rejected\n",
        "    \"\"\"\n",
        "    return True  # In negative rejection testbed, all should be rejected\n",
        "\n",
        "def evaluate_negative_rejection_rgb(dataset, model, sample_size, max_docs):\n",
        "    \"\"\"RGB paper-style negative rejection evaluation\"\"\"\n",
        "    print(f\"\\nEvaluating {model} for RGB-style negative rejection\")\n",
        "    print(f\"Sample size: {sample_size}, Max docs per question: {max_docs}\")\n",
        "\n",
        "    # Filter samples with sufficient negative documents\n",
        "    filtered_samples = []\n",
        "    for sample in dataset:\n",
        "        negative_docs = sample.get('negative', [])\n",
        "        if len(negative_docs) >= max_docs:\n",
        "            filtered_samples.append(sample)\n",
        "        if len(filtered_samples) >= sample_size:\n",
        "            break\n",
        "\n",
        "    if len(filtered_samples) < sample_size:\n",
        "        print(f\"Warning: Only found {len(filtered_samples)} quality samples (target: {sample_size})\")\n",
        "\n",
        "    actual_sample_size = min(sample_size, len(filtered_samples))\n",
        "    samples_to_process = filtered_samples[:actual_sample_size]\n",
        "\n",
        "    results = []\n",
        "    exact_rejections = 0  # Rej metric\n",
        "    semantic_rejections = 0  # Rej* metric\n",
        "    api_errors = 0\n",
        "    response_diversity = set()  # Track response diversity\n",
        "\n",
        "    progress_bar = tqdm(samples_to_process, desc=f\"{model}\")\n",
        "\n",
        "    for i, sample in enumerate(samples_to_process):\n",
        "        question = sample[\"query\"]\n",
        "        negative_docs = sample.get(\"negative\", [])\n",
        "\n",
        "        # Sample ONLY negative documents (RGB negative rejection setup)\n",
        "        docs = random.sample(negative_docs, min(max_docs, len(negative_docs)))\n",
        "\n",
        "        # Get model response\n",
        "        system_prompt = get_rgb_system_prompt()\n",
        "        predicted = query_model(question, docs, model, system_prompt=system_prompt)\n",
        "\n",
        "        if \"API_ERROR\" in predicted:\n",
        "            api_errors += 1\n",
        "            results.append({\n",
        "                \"sample_id\": i + 1,\n",
        "                \"model\": model,\n",
        "                \"question\": question,\n",
        "                \"predicted\": predicted,\n",
        "                \"exact_rejection\": False,\n",
        "                \"semantic_rejection\": False,\n",
        "                \"should_reject\": True,\n",
        "                \"docs_used\": len(docs),\n",
        "                \"documents\": \" ||| \".join(docs[:2])\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # RGB evaluation metrics\n",
        "        exact_rejection = check_exact_rejection(predicted)\n",
        "        semantic_rejection = check_semantic_rejection(predicted)\n",
        "        should_reject = should_be_rejected_rgb(question, docs)  # Always True for negative rejection\n",
        "\n",
        "        if exact_rejection:\n",
        "            exact_rejections += 1\n",
        "        if semantic_rejection:\n",
        "            semantic_rejections += 1\n",
        "\n",
        "        # Track response diversity\n",
        "        response_diversity.add(predicted.strip()[:100])  # First 100 chars\n",
        "\n",
        "        results.append({\n",
        "            \"sample_id\": i + 1,\n",
        "            \"model\": model,\n",
        "            \"question\": question,\n",
        "            \"predicted\": predicted,\n",
        "            \"exact_rejection\": exact_rejection,\n",
        "            \"semantic_rejection\": semantic_rejection,\n",
        "            \"should_reject\": should_reject,\n",
        "            \"docs_used\": len(docs),\n",
        "            \"documents\": \" ||| \".join(docs[:2])\n",
        "        })\n",
        "\n",
        "        # Update progress\n",
        "        valid_samples = len([r for r in results if \"API_ERROR\" not in r[\"predicted\"]])\n",
        "        if valid_samples > 0:\n",
        "            exact_rate = (exact_rejections / valid_samples) * 100\n",
        "            semantic_rate = (semantic_rejections / valid_samples) * 100\n",
        "            progress_bar.set_postfix({\n",
        "                'Exact': f'{exact_rate:.1f}%',\n",
        "                'Semantic': f'{semantic_rate:.1f}%',\n",
        "                'Errors': api_errors\n",
        "            })\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    # Calculate final metrics\n",
        "    valid_results = [r for r in results if \"API_ERROR\" not in r[\"predicted\"]]\n",
        "    total_questions = len(valid_results)\n",
        "\n",
        "    if total_questions == 0:\n",
        "        print(f\"No valid results for {model} due to API errors\")\n",
        "        return {\"exact_rate\": 0, \"semantic_rate\": 0, \"diversity\": 0}, results\n",
        "\n",
        "    exact_rejection_rate = (exact_rejections / total_questions) * 100\n",
        "    semantic_rejection_rate = (semantic_rejections / total_questions) * 100\n",
        "    diversity_score = len(response_diversity)\n",
        "\n",
        "    print(f\"\\n{model} Results (RGB-style):\")\n",
        "    print(f\"  Total Questions: {total_questions}\")\n",
        "    print(f\"  Exact Rejections (Rej): {exact_rejections} ({exact_rejection_rate:.1f}%)\")\n",
        "    print(f\"  Semantic Rejections (Rej*): {semantic_rejections} ({semantic_rejection_rate:.1f}%)\")\n",
        "    print(f\"  Response Diversity: {diversity_score} unique responses\")\n",
        "    if api_errors > 0:\n",
        "        print(f\"  API Errors: {api_errors}\")\n",
        "\n",
        "    return {\n",
        "        \"exact_rate\": exact_rejection_rate,\n",
        "        \"semantic_rate\": semantic_rejection_rate,\n",
        "        \"diversity\": diversity_score\n",
        "    }, results"
      ],
      "metadata": {
        "id": "2uF_yvvq1Bso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULT SAVING AND ANALYSIS FUNCTIONS**"
      ],
      "metadata": {
        "id": "4RjMJ-e_33Lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_rgb_results_by_model_and_noise(summary_data, detail_data, output_prefix):\n",
        "    \"\"\"Save RGB evaluation results separated by model and including noise analysis\"\"\"\n",
        "    if not summary_data or not detail_data:\n",
        "        print(\"No data to save\")\n",
        "        return None, None\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    detail_df = pd.DataFrame(detail_data)\n",
        "\n",
        "    # Get unique models\n",
        "    models = summary_df['model'].unique() if len(summary_df) > 0 else detail_df['model'].unique()\n",
        "\n",
        "    saved_files = {\"summary\": [], \"detail\": [], \"analysis\": []}\n",
        "\n",
        "    # Save files for each model individually\n",
        "    for model in models:\n",
        "        # Clean model name for filename\n",
        "        safe_model_name = model.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
        "\n",
        "        # Filter data for current model\n",
        "        model_summary = summary_df[summary_df['model'] == model] if len(summary_df) > 0 else pd.DataFrame()\n",
        "        model_detail = detail_df[detail_df['model'] == model]\n",
        "\n",
        "        # Create filenames\n",
        "        summary_file = f\"{output_prefix}_{safe_model_name}_summary.csv\"\n",
        "        detail_file = f\"{output_prefix}_{safe_model_name}_details.csv\"\n",
        "\n",
        "        # Save individual model files\n",
        "        if len(model_summary) > 0:\n",
        "            model_summary.to_csv(summary_file, index=False)\n",
        "            saved_files[\"summary\"].append(summary_file)\n",
        "\n",
        "        if len(model_detail) > 0:\n",
        "            model_detail.to_csv(detail_file, index=False)\n",
        "            saved_files[\"detail\"].append(detail_file)\n",
        "\n",
        "        print(f\"Saved {model} results:\")\n",
        "        print(f\"  - Summary: {summary_file}\")\n",
        "        print(f\"  - Details: {detail_file}\")\n",
        "\n",
        "    # Also save combined files\n",
        "    combined_summary_file = f\"{output_prefix}_all_models_summary.csv\"\n",
        "    combined_detail_file = f\"{output_prefix}_all_models_details.csv\"\n",
        "\n",
        "    if len(summary_df) > 0:\n",
        "        summary_df.to_csv(combined_summary_file, index=False)\n",
        "    detail_df.to_csv(combined_detail_file, index=False)\n",
        "\n",
        "    print(f\"\\nCombined files saved:\")\n",
        "    if len(summary_df) > 0:\n",
        "        print(f\"  - All Summary: {combined_summary_file}\")\n",
        "    print(f\"  - All Details: {combined_detail_file}\")\n",
        "\n",
        "    return saved_files, {\"summary\": combined_summary_file, \"detail\": combined_detail_file}\n",
        "\n",
        "def create_downloadable_package(output_prefix):\n",
        "    \"\"\"Create a downloadable zip package with all results\"\"\"\n",
        "    zip_filename = f\"{output_prefix}_complete_results.zip\"\n",
        "\n",
        "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "        # Add all CSV files\n",
        "        csv_files = glob.glob(f\"{output_prefix}*.csv\")\n",
        "        for csv_file in csv_files:\n",
        "            zipf.write(csv_file, os.path.basename(csv_file))\n",
        "\n",
        "        # Add results directory if it exists\n",
        "        results_dir = f\"{output_prefix}_results\"\n",
        "        if os.path.exists(results_dir):\n",
        "            for root, dirs, files in os.walk(results_dir):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, os.path.dirname(results_dir))\n",
        "                    zipf.write(file_path, arcname)\n",
        "\n",
        "    print(f\"\\nDownloadable package created: {zip_filename}\")\n",
        "    return zip_filename"
      ],
      "metadata": {
        "id": "mORQm7iI1KdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAIN EXECUTION FUNCTIONS**"
      ],
      "metadata": {
        "id": "PA6o6lK74Bbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_information_integration_evaluation():\n",
        "    \"\"\"Run Information Integration evaluation\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"INFORMATION INTEGRATION EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    set_seeds(RANDOM_SEED)\n",
        "\n",
        "    # Load data\n",
        "    info_int_data = load_data(INFO_INT_FILE)\n",
        "    print(f\"Loaded {len(info_int_data)} Information Integration samples\")\n",
        "\n",
        "    all_summary_data = []\n",
        "    all_detail_data = []\n",
        "\n",
        "    for model in MODELS_TO_EVALUATE:\n",
        "        for noise_ratio in NOISE_RATIOS:\n",
        "            random.seed(RANDOM_SEED + int(noise_ratio * 10))\n",
        "\n",
        "            stats, details = evaluate_rgb_information_integration(\n",
        "                info_int_data, model, SAMPLE_SIZE, MAX_DOCS, noise_ratio\n",
        "            )\n",
        "\n",
        "            if stats[\"total\"] > 0:\n",
        "                all_summary_data.append({\n",
        "                    \"model\": model,\n",
        "                    \"noise_ratio\": noise_ratio,\n",
        "                    \"total_samples\": stats[\"total\"],\n",
        "                    \"full_success\": stats[\"FULL_SUCCESS\"],\n",
        "                    \"partial_success\": stats[\"PARTIAL_SUCCESS\"],\n",
        "                    \"failure\": stats[\"FAILURE\"],\n",
        "                    \"rejected\": stats[\"REJECTED\"],\n",
        "                    \"factual_error_detected\": stats[\"FACTUAL_ERROR_DETECTED\"],\n",
        "                    \"error\": stats[\"ERROR\"],\n",
        "                    \"full_success_rate\": (stats[\"FULL_SUCCESS\"] / stats[\"total\"] * 100),\n",
        "                    \"partial_success_rate\": (stats[\"PARTIAL_SUCCESS\"] / stats[\"total\"] * 100),\n",
        "                    \"overall_success_rate\": ((stats[\"FULL_SUCCESS\"] + stats[\"PARTIAL_SUCCESS\"]) / stats[\"total\"] * 100),\n",
        "                    \"failure_rate\": (stats[\"FAILURE\"] / stats[\"total\"] * 100),\n",
        "                    \"rejection_rate\": (stats[\"REJECTED\"] / stats[\"total\"] * 100),\n",
        "                    \"temperature\": TEMPERATURE\n",
        "                })\n",
        "\n",
        "            all_detail_data.extend(details)\n",
        "\n",
        "    # Save results\n",
        "    if all_detail_data:\n",
        "        save_rgb_results_by_model_and_noise(\n",
        "            all_summary_data, all_detail_data, \"rgb_information_integration\"\n",
        "        )\n",
        "        return pd.DataFrame(all_summary_data), pd.DataFrame(all_detail_data)\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def run_factual_accuracy_evaluation():\n",
        "    \"\"\"Run Factual Accuracy evaluation\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"FACTUAL ACCURACY EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    set_seeds(RANDOM_SEED)\n",
        "\n",
        "    # Load data\n",
        "    fact_data = load_data(FACT_FILE)\n",
        "    print(f\"Loaded {len(fact_data)} factual accuracy samples\")\n",
        "\n",
        "    all_summary_data = []\n",
        "    all_detail_data = []\n",
        "\n",
        "    for model in MODELS_TO_EVALUATE:\n",
        "        scenario_stats, details = evaluate_rgb_factual_accuracy(\n",
        "            fact_data, model, SAMPLE_SIZE, MAX_DOCS\n",
        "        )\n",
        "\n",
        "        # Create summary data\n",
        "        for scenario, stats in scenario_stats.items():\n",
        "            if stats[\"total\"] > 0:\n",
        "                all_summary_data.append({\n",
        "                    \"model\": model,\n",
        "                    \"scenario\": scenario,\n",
        "                    \"total_samples\": stats[\"total\"],\n",
        "                    \"correct\": stats[\"CORRECT\"],\n",
        "                    \"incorrect\": stats[\"INCORRECT\"],\n",
        "                    \"rejected\": stats[\"REJECTED\"],\n",
        "                    \"partial\": stats[\"PARTIAL\"],\n",
        "                    \"no_answer\": stats[\"NO_ANSWER\"],\n",
        "                    \"error\": stats[\"ERROR\"],\n",
        "                    \"accuracy\": (stats[\"CORRECT\"] / stats[\"total\"] * 100),\n",
        "                    \"error_rate\": (stats[\"INCORRECT\"] / stats[\"total\"] * 100),\n",
        "                    \"rejection_rate\": (stats[\"REJECTED\"] / stats[\"total\"] * 100)\n",
        "                })\n",
        "\n",
        "        all_detail_data.extend(details)\n",
        "\n",
        "    # Save results\n",
        "    if all_summary_data and all_detail_data:\n",
        "        save_rgb_results_by_model_and_noise(\n",
        "            all_summary_data, all_detail_data, \"rgb_factual_accuracy\"\n",
        "        )\n",
        "        return pd.DataFrame(all_summary_data), pd.DataFrame(all_detail_data)\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def run_noise_robustness_evaluation():\n",
        "    \"\"\"Run Noise Robustness evaluation\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"NOISE ROBUSTNESS EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    set_seeds(RANDOM_SEED)\n",
        "\n",
        "    # Load data\n",
        "    en_data = load_data(EN_FILE)\n",
        "    zh_data = load_data(ZH_FILE)\n",
        "    print(f\"Loaded {len(en_data)} English samples, {len(zh_data)} Chinese samples\")\n",
        "\n",
        "    summary_data = []\n",
        "    detail_data = []\n",
        "\n",
        "    for model in MODELS_TO_EVALUATE:\n",
        "        en_results, en_details = eval_noise_test(en_data, model, \"en\", SAMPLE_SIZE, base_seed=42)\n",
        "        zh_results, zh_details = eval_noise_test(zh_data, model, \"zh\", SAMPLE_SIZE, base_seed=42)\n",
        "\n",
        "        for ratio in NOISE_RATIOS_FULL:\n",
        "            summary_data.append({\n",
        "                \"model\": model,\n",
        "                \"language\": \"en\",\n",
        "                \"noise_ratio\": ratio,\n",
        "                \"accuracy\": en_results.get(ratio, 0)\n",
        "            })\n",
        "            summary_data.append({\n",
        "                \"model\": model,\n",
        "                \"language\": \"zh\",\n",
        "                \"noise_ratio\": ratio,\n",
        "                \"accuracy\": zh_results.get(ratio, 0)\n",
        "            })\n",
        "\n",
        "        detail_data.extend(en_details)\n",
        "        detail_data.extend(zh_details)\n",
        "\n",
        "    # Save results\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    detail_df = pd.DataFrame(detail_data)\n",
        "\n",
        "    summary_df.to_csv(\"noise_evaluation_summary.csv\", index=False)\n",
        "    detail_df.to_csv(\"noise_evaluation_details.csv\", index=False)\n",
        "\n",
        "    return summary_df, detail_df\n",
        "\n",
        "def run_negative_rejection_evaluation():\n",
        "    \"\"\"Run Negative Rejection evaluation\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"NEGATIVE REJECTION EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    set_seeds(RANDOM_SEED)\n",
        "\n",
        "    # Load data\n",
        "    en_data = load_data(EN_FILE)\n",
        "    print(f\"Loaded {len(en_data)} English samples\")\n",
        "\n",
        "    summary_data = []\n",
        "    detail_data = []\n",
        "\n",
        "    for model in MODELS_TO_EVALUATE:\n",
        "        metrics, details = evaluate_negative_rejection_rgb(\n",
        "            en_data, model, SAMPLE_SIZE, MAX_DOCS\n",
        "        )\n",
        "\n",
        "        if details:\n",
        "            summary_data.append({\n",
        "                \"model\": model,\n",
        "                \"exact_rejection_rate\": metrics[\"exact_rate\"],\n",
        "                \"semantic_rejection_rate\": metrics[\"semantic_rate\"],\n",
        "                \"response_diversity\": metrics[\"diversity\"],\n",
        "                \"samples_evaluated\": len(details)\n",
        "            })\n",
        "\n",
        "            detail_data.extend(details)\n",
        "\n",
        "    # Save results\n",
        "    if summary_data:\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        detail_df = pd.DataFrame(detail_data)\n",
        "\n",
        "        summary_df.to_csv(\"rgb_negative_rejection_summary.csv\", index=False)\n",
        "        detail_df.to_csv(\"rgb_negative_rejection_details.csv\", index=False)\n",
        "\n",
        "        return summary_df, detail_df\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def run_all_evaluations():\n",
        "    \"\"\"Run all RGB evaluations\"\"\"\n",
        "    print(\"RGB EVALUATION SUITE - COMPREHENSIVE TESTING\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Models to evaluate: {MODELS_TO_EVALUATE}\")\n",
        "    print(f\"Sample size per evaluation: {SAMPLE_SIZE}\")\n",
        "    print(f\"Temperature: {TEMPERATURE}\")\n",
        "    print(f\"Random seed: {RANDOM_SEED}\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Run all evaluations\n",
        "    print(\"\\n1. Running Information Integration Evaluation...\")\n",
        "    results['info_int'] = run_information_integration_evaluation()\n",
        "\n",
        "    print(\"\\n2. Running Factual Accuracy Evaluation...\")\n",
        "    results['fact_acc'] = run_factual_accuracy_evaluation()\n",
        "\n",
        "    print(\"\\n3. Running Noise Robustness Evaluation...\")\n",
        "    results['noise_rob'] = run_noise_robustness_evaluation()\n",
        "\n",
        "    print(\"\\n4. Running Negative Rejection Evaluation...\")\n",
        "    results['neg_rej'] = run_negative_rejection_evaluation()\n",
        "\n",
        "    # Create comprehensive package\n",
        "    print(\"\\nCreating comprehensive results package...\")\n",
        "    create_downloadable_package(\"rgb_comprehensive_evaluation\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ALL EVALUATIONS COMPLETED\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Check the generated CSV files and zip package for detailed results.\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "i5_YaMHk1QTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTEBOOK EXECUTION**"
      ],
      "metadata": {
        "id": "M_U_OSVP4G0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # You can run individual evaluations or all at once\n",
        "\n",
        "    # Option 1: Run all evaluations\n",
        "    results = run_all_evaluations()\n",
        "\n",
        "    # Option 2: Run individual evaluations (uncomment as needed)\n",
        "    # info_int_summary, info_int_details = run_information_integration_evaluation()\n",
        "    # fact_acc_summary, fact_acc_details = run_factual_accuracy_evaluation()\n",
        "    # noise_rob_summary, noise_rob_details = run_noise_robustness_evaluation()\n",
        "    # neg_rej_summary, neg_rej_details = run_negative_rejection_evaluation()"
      ],
      "metadata": {
        "id": "aTpnfh0p1Z-z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}